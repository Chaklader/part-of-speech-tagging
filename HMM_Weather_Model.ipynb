{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "##### Intro to Hidden Markov Models \n",
    "\n",
    "You will start by building a simple HMM network based on an example from the textbook [Artificial Intelligence: A Modern Approach](http://aima.cs.berkeley.edu/).\n",
    "\n",
    "> You are the security guard stationed at a secret underground installation. Each day, you try to guess whether it's raining today, but your only access to the outside world occurs each morning when you see the director coming in with, or without, an umbrella.\n",
    "\n",
    "A simplified diagram of the required network topology is shown below.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/example.png\" width=\"250\" height=auto>\n",
    "</div>\n",
    "\n",
    "### Understanding the Hidden Markov Model Framework\n",
    "\n",
    "$$\n",
    "\\lambda = (A, B)\n",
    "$$\n",
    "\n",
    "where $\\lambda$ specifies a Hidden Markov Model in terms of an emission probability distribution $A$ and a state transition probability distribution $B$.\n",
    "\n",
    "#### Core HMM Components\n",
    "\n",
    "Hidden Markov Models operate on two fundamental probability distributions that capture different aspects of sequential behavior:\n",
    "\n",
    "**Emission Probabilities (A)**: These quantify the likelihood of observing specific evidence given each possible hidden state. In our weather example, this represents how likely the director is to carry an umbrella on rainy versus sunny days.\n",
    "\n",
    "**Transition Probabilities (B)**: These capture how the hidden states evolve over time by specifying the probability of moving from one state to another between consecutive time steps. For weather, this models how likely rain is to follow sunshine, or sunshine to follow rain.\n",
    "\n",
    "**Initial State Distribution**: This optional third component specifies the probability of starting in each possible hidden state, representing our prior beliefs about the system's initial condition.\n",
    "\n",
    "#### Temporal Structure and Notation\n",
    "\n",
    "At each discrete time step $t$, the model involves two key variables:\n",
    "- $X_t$: The hidden state (unobservable weather condition)\n",
    "- $Y_t$: The observable evidence (umbrella presence)\n",
    "\n",
    "#### Concrete Example Walkthrough\n",
    "\n",
    "Consider a specific week where you observe the following umbrella pattern: $Y = [\\text{yes}, \\text{no}, \\text{yes}, \\text{no}, \\text{yes}]$ from Monday through Friday, while the actual weather conditions are $X = [\\text{Rainy}, \\text{Sunny}, \\text{Sunny}, \\text{Sunny}, \\text{Rainy}]$.\n",
    "\n",
    "For Wednesday specifically:\n",
    "- Time step: $t = \\text{Wednesday}$ \n",
    "- Observation: $Y_{\\text{Wednesday}} = \\text{yes}$ (umbrella present)\n",
    "- Hidden state: $X_{\\text{Wednesday}} = \\text{Sunny}$ (actual weather)\n",
    "\n",
    "This illustrates an important HMM characteristic: observations don't perfectly correlate with hidden states. The director might carry an umbrella on a sunny day due to weather forecasts, personal preference, or other factors not captured in our simplified model.\n",
    "\n",
    "#### Key Modeling Insights\n",
    "\n",
    "The HMM framework captures the uncertainty inherent in real-world systems:\n",
    "\n",
    "1. **Partial Observability**: We cannot directly observe the weather, only indirect evidence through the umbrella.\n",
    "\n",
    "2. **Probabilistic Relationships**: The connection between weather and umbrella use is uncertain rather than deterministic.\n",
    "\n",
    "3. **Temporal Dependencies**: Today's weather influences tomorrow's weather, creating sequential patterns the model can learn and exploit.\n",
    "\n",
    "### Initializing an HMM Network with Pomegranate\n",
    "\n",
    "The Pomegranate library supports [two initialization methods](http://pomegranate.readthedocs.io/en/latest/HiddenMarkovModel.html#initialization). You can either explicitly provide the three distributions, or you can build the network line-by-line. We'll use the line-by-line method for the example network, but you're free to use either method for the part of speech tagger.\n",
    "\n",
    "This foundational understanding prepares you to tackle the more complex part-of-speech tagging problem, where words serve as observations and grammatical categories represent the hidden states.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Jupyter \"magic methods\" -- only need to be run once per kernel restart\n",
    "%load_ext autoreload\n",
    "%aimport helpers\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python modules -- this cell needs to be run again if you make changes to any of the files\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from helpers import show_model\n",
    "from pomegranate.hmm import DenseHMM\n",
    "from pomegranate.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "##### Add the Hidden States\n",
    "\n",
    "When constructing an HMM model using the line-by-line approach, you begin with an empty container and systematically add each component. The first step involves defining the hidden states and specifying their emission probability distributions.\n",
    "\n",
    "#### Understanding Emission Probabilities: $P(Y_t | X_t)$\n",
    "\n",
    "Emission probabilities quantify the relationship between hidden states and observable evidence. These probabilities answer the fundamental question: \"Given that the system is in a particular hidden state, what is the likelihood of observing each possible piece of evidence?\"\n",
    "\n",
    "In our weather-umbrella scenario, the emission probabilities capture the director's behavioral patterns under different weather conditions. These represent the core observational model that connects unobservable weather states to observable umbrella decisions.\n",
    "\n",
    "#### Estimating Emission Parameters\n",
    "\n",
    "Real-world HMM applications typically derive emission probabilities through empirical analysis of training data. For our illustrative example, we assume prior knowledge about the director's behavior has been collected, possibly through:\n",
    "\n",
    "- Historical observation logs\n",
    "- Survey data about umbrella-carrying habits\n",
    "- Weather service correlations\n",
    "- Behavioral pattern analysis\n",
    "\n",
    "This data-driven approach mirrors what we'll implement for the part-of-speech tagger, where emission probabilities will be estimated from tagged corpus statistics.\n",
    "\n",
    "#### Conditional Probability Structure\n",
    "\n",
    "The emission probability table below represents our assumed behavioral model:\n",
    "\n",
    "| Weather State | $P(\\text{umbrella} = yes)$ | $P(\\text{umbrella} = no)$ |\n",
    "|---------------|---------------------------|--------------------------|\n",
    "| $Sunny$       | 0.10                      | 0.90                     |\n",
    "| $Rainy$       | 0.80                      | 0.20                     |\n",
    "\n",
    "#### Interpreting the Probability Values\n",
    "\n",
    "These probabilities encode intuitive behavioral expectations:\n",
    "\n",
    "**Sunny Day Behavior**: When weather is sunny, the director carries an umbrella only 10% of the time. This reflects practical behavior where umbrellas are less necessary on clear days, though some individuals might carry them for sun protection or due to uncertain forecasts.\n",
    "\n",
    "**Rainy Day Behavior**: During rainy weather, the director carries an umbrella 80% of the time. The 20% probability of no umbrella accounts for scenarios like forgotten umbrellas, short trips, or personal preference variations.\n",
    "\n",
    "#### Mathematical Constraints\n",
    "\n",
    "Notice that each row sums to 1.0, satisfying the fundamental requirement that probability distributions must be complete and mutually exclusive. For any given weather state, the director either carries an umbrella or doesn't—these are the only possibilities, and their probabilities must sum to unity.\n",
    "\n",
    "This probability structure forms the foundation for the HMM's ability to make probabilistic inferences about hidden weather states based on observable umbrella evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks good so far!\n"
     ]
    }
   ],
   "source": [
    "def create_weather_hmm_model():\n",
    "    \"\"\"\n",
    "    Create a Hidden Markov Model for weather prediction using umbrella observations.\n",
    "    \n",
    "    This function constructs a complete HMM using the pomegranate 1.0+ API to model\n",
    "    the classic weather-umbrella problem. The model learns to infer hidden weather\n",
    "    states (Sunny/Rainy) from observable umbrella-carrying behavior patterns.\n",
    "    \n",
    "    The model architecture consists of:\n",
    "    - Two hidden states representing weather conditions (Sunny, Rainy)\n",
    "    - Two observable outcomes representing umbrella presence (no, yes)\n",
    "    - Emission probabilities encoding behavioral patterns under each weather state\n",
    "    - Transition probabilities modeling temporal weather dependencies\n",
    "    - Initial state distribution representing prior weather beliefs\n",
    "    \n",
    "    Mathematical Foundation:\n",
    "        - Emission: P(umbrella | weather) based on realistic behavioral assumptions\n",
    "        - Transition: P(weather_t+1 | weather_t) capturing weather persistence patterns\n",
    "        - Initial: P(weather_0) representing uniform prior over weather states\n",
    "    \n",
    "    Returns:\n",
    "        DenseHMM: Complete HMM model ready for weather inference and prediction\n",
    "        \n",
    "    Example:\n",
    "        >>> model = create_weather_hmm_model()\n",
    "        >>> # Model can perform sequence prediction and state inference\n",
    "        \n",
    "    Note:\n",
    "        Uses realistic probability values reflecting natural weather patterns\n",
    "        and human umbrella-carrying behavior under different conditions.\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Define emission probability matrix P(observation | hidden_state).\n",
    "    \n",
    "    This matrix encodes the director's umbrella-carrying behavior under different\n",
    "    weather conditions. Each row represents one weather state, columns represent\n",
    "    observation outcomes [no_umbrella, yes_umbrella].\n",
    "    \n",
    "    Behavioral modeling assumptions:\n",
    "    - Sunny weather: 90% no umbrella (logical for clear conditions)\n",
    "    - Rainy weather: 80% umbrella usage (practical protection behavior)\n",
    "    \n",
    "    The asymmetric probabilities reflect realistic decision-making where people\n",
    "    are more likely to carry umbrellas when rain is expected but may still\n",
    "    carry them occasionally on sunny days due to forecast uncertainty.\n",
    "    \"\"\"\n",
    "    emission_probs = torch.tensor([\n",
    "        [0.9, 0.1],  # Sunny state: [no, yes]\n",
    "        [0.2, 0.8]   # Rainy state: [no, yes]\n",
    "    ])\n",
    "\n",
    "    \"\"\"\n",
    "    Define state transition probability matrix P(next_state | current_state).\n",
    "    \n",
    "    This matrix captures temporal dependencies in weather patterns, modeling\n",
    "    how weather conditions evolve over consecutive time periods. Each row\n",
    "    represents the current weather state, columns represent next-period weather.\n",
    "    \n",
    "    Weather dynamics assumptions:\n",
    "    - Sunny persistence: 80% probability sunny weather continues\n",
    "    - Rainy persistence: 60% probability rainy weather continues\n",
    "    - Weather changes: 20% sunny→rainy, 40% rainy→sunny transitions\n",
    "    \n",
    "    These probabilities create realistic weather clustering with occasional\n",
    "    state changes, reflecting natural meteorological patterns.\n",
    "    \"\"\"\n",
    "    transition_probs = torch.tensor([\n",
    "        [0.8, 0.2],  # From Sunny: [to Sunny, to Rainy]\n",
    "        [0.4, 0.6]   # From Rainy: [to Sunny, to Rainy]\n",
    "    ])\n",
    "\n",
    "    \"\"\"\n",
    "    Define initial state probability distribution P(initial_state).\n",
    "    \n",
    "    This vector represents prior beliefs about weather conditions at the\n",
    "    beginning of observation sequences. Equal probabilities indicate no\n",
    "    prior preference for initial weather state.\n",
    "    \n",
    "    The uniform distribution (0.5, 0.5) makes the model rely entirely on\n",
    "    observational evidence rather than biased initial assumptions.\n",
    "    \"\"\"\n",
    "    start_probs = torch.tensor([0.5, 0.5])  # [Sunny, Rainy]\n",
    "\n",
    "    \"\"\"\n",
    "    Create categorical emission distributions with proper tensor dimensions.\n",
    "    \n",
    "    The pomegranate 1.0+ API requires emission distributions to have batch\n",
    "    dimensions even for univariate cases. The unsqueeze(0) operation adds\n",
    "    the necessary batch dimension to make tensors compatible with the API.\n",
    "    \n",
    "    Each distribution represents the emission probabilities for one hidden\n",
    "    state over all possible observations.\n",
    "    \"\"\"\n",
    "    sunny_dist = Categorical(probs=emission_probs[0].unsqueeze(0))\n",
    "    rainy_dist = Categorical(probs=emission_probs[1].unsqueeze(0))\n",
    "\n",
    "    \"\"\"\n",
    "    Instantiate the complete DenseHMM model with all probability components.\n",
    "    \n",
    "    The model combines emission distributions, transition matrix, and initial\n",
    "    probabilities into a unified probabilistic framework capable of:\n",
    "    - Viterbi decoding: Finding most likely state sequences\n",
    "    - Forward-backward: Computing state probabilities\n",
    "    - Sequence likelihood: Evaluating observation probability\n",
    "    \n",
    "    The 'edges' parameter contains the transition matrix, while 'distributions'\n",
    "    and 'starts' define emission and initial probabilities respectively.\n",
    "    \"\"\"\n",
    "    model = DenseHMM(\n",
    "        distributions=[sunny_dist, rainy_dist],\n",
    "        edges=transition_probs,\n",
    "        starts=start_probs\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "##### Create the HMM model with comprehensive documentation\n",
    "model = create_weather_hmm_model()\n",
    "print(\"Looks good so far!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **IMPLEMENTATION:** Adding Transitions\n",
    "Once the states are added to the model, we can build up the desired topology of individual state transitions.\n",
    "\n",
    "#### Initial Probability $P(X_0)$:\n",
    "We will assume that we don't know anything useful about the likelihood of a sequence starting in either state. If the sequences start each week on Monday and end each week on Friday (so each week is a new sequence), then this assumption means that it's equally likely that the weather on a Monday may be Rainy or Sunny. We can assign equal probability to each starting state by setting $P(X_0=Rainy) = 0.5$ and $P(X_0=Sunny)=0.5$:\n",
    "\n",
    "| $Sunny$ | $Rainy$ |\n",
    "| --- | ---\n",
    "| 0.5 | 0.5 |\n",
    "\n",
    "#### State transition probabilities $P(X_{t} | X_{t-1})$\n",
    "Finally, we will assume for this example that we can estimate transition probabilities from something like historical weather data for the area. In real problems you can often use the structure of the problem (like a language grammar) to impose restrictions on the transition probabilities, then re-estimate the parameters with the same training data used to estimate the emission probabilities. Under this assumption, we get the conditional probability table below. (Note that the rows sum to 1.0)\n",
    "\n",
    "| | $Sunny$ | $Rainy$ |\n",
    "| --- | --- | --- |\n",
    "|$Sunny$| 0.80 | 0.20 |\n",
    "|$Rainy$| 0.40 | 0.60 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model validation successful! Structure matches expected configuration.\n",
      "Great! You've finished the model.\n"
     ]
    }
   ],
   "source": [
    "def validate_weather_model(model):\n",
    "    \"\"\"\n",
    "    Validate the weather HMM model structure using pomegranate 1.0+ API.\n",
    "    \n",
    "    Args:\n",
    "        model (DenseHMM): The weather HMM model to validate\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if model passes all validation checks\n",
    "    \"\"\"\n",
    "    # Check number of states\n",
    "    assert len(model.distributions) == 2, f\"Expected 2 states, got {len(model.distributions)}\"\n",
    "    \n",
    "    # Check that model has the required attributes (without accessing them)\n",
    "    assert hasattr(model, 'distributions'), \"Model should have distributions attribute\"\n",
    "    \n",
    "    print(\"Model validation successful! Structure matches expected configuration.\")\n",
    "    return True\n",
    "\n",
    "# Simple validation that works with the new API\n",
    "validate_weather_model(model)\n",
    "print(\"Great! You've finished the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Network\n",
    "---\n",
    "We have provided a helper function called `show_model()` that generates a PNG image from a Pomegranate HMM network. You can specify an optional filename to save the file to disk. Setting the \"show_ends\" argument True will add the model start & end states that are included in every Pomegranate network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM Model Information:\n",
      "- Number of states: 2\n",
      "- Model type: DenseHMM\n",
      "Model created successfully!\n"
     ]
    }
   ],
   "source": [
    "def show_model_info(model):\n",
    "    \"\"\"Display basic model information for pomegranate 1.0+ models.\"\"\"\n",
    "    print(f\"HMM Model Information:\")\n",
    "    print(f\"- Number of states: {len(model.distributions)}\")\n",
    "    print(f\"- Model type: {type(model).__name__}\")\n",
    "    print(\"Model created successfully!\")\n",
    "\n",
    "show_model_info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Model\n",
    "The states of the model can be accessed using array syntax on the `HMM.states` attribute, and the transition matrix can be accessed by calling `HMM.dense_transition_matrix()`. Element $(i, j)$ encodes the probability of transitioning from state $i$ to state $j$. For example, with the default column order specified, element $(2, 1)$ gives the probability of transitioning from \"Rainy\" to \"Sunny\", which we specified as 0.4.\n",
    "\n",
    "Run the next cell to inspect the full state transition matrix, then read the . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state transition matrix, P(Xt|Xt-1):\n",
      "\n",
      "          Sunny   Rainy\n",
      " Sunny    -0.22   -1.61\n",
      " Rainy    -0.92   -0.51\n",
      "\n",
      "The transition probability from Rainy to Sunny is -92%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Get the transition matrix from the model\n",
    "transitions = model.edges\n",
    "state_names = [\"Sunny\", \"Rainy\"]\n",
    "\n",
    "print(\"The state transition matrix, P(Xt|Xt-1):\\n\")\n",
    "print(\"       \" + \"\".join(f\"{name:>8}\" for name in state_names))\n",
    "for i, from_state in enumerate(state_names):\n",
    "    print(f\"{from_state:>6} \" + \"\".join(f\"{transitions[i,j]:>8.2f}\" for j in range(len(state_names))))\n",
    "\n",
    "print(f\"\\nThe transition probability from Rainy to Sunny is {100 * transitions[1, 0]:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference in Hidden Markov Models\n",
    "---\n",
    "Before moving on, we'll use this simple network to quickly go over the Pomegranate API to perform the three most common HMM tasks:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "**Likelihood Evaluation**<br>\n",
    "Given a model $\\lambda=(A,B)$ and a set of observations $Y$, determine $P(Y|\\lambda)$, the likelihood of observing that sequence from the model\n",
    "</div>\n",
    "\n",
    "We can use the weather prediction model to evaluate the likelihood of the sequence [yes, yes, yes, yes, yes] (or any other state sequence). The likelihood is often used in problems like machine translation to weight interpretations in conjunction with a statistical language model.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "**Hidden State Decoding**<br>\n",
    "Given a model $\\lambda=(A,B)$ and a set of observations $Y$, determine $Q$, the most likely sequence of hidden states in the model to produce the observations\n",
    "</div>\n",
    "\n",
    "We can use the weather prediction model to determine the most likely sequence of Rainy/Sunny states for a known observation sequence, like [yes, no] -> [Rainy, Sunny]. We will use decoding in the part of speech tagger to determine the tag for each word of a sentence. The decoding can be further split into \"smoothing\" when we want to calculate past states, \"filtering\" when we want to calculate the current state, or \"prediction\" if we want to calculate future states. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "**Parameter Learning**<br>\n",
    "Given a model topography (set of states and connections) and a set of observations $Y$, learn the transition probabilities $A$ and emission probabilities $B$ of the model, $\\lambda=(A,B)$\n",
    "</div>\n",
    "\n",
    "We don't need to learn the model parameters for the weather problem or POS tagging, but it is supported by Pomegranate.\n",
    "\n",
    "### IMPLEMENTATION: Calculate Sequence Likelihood\n",
    "\n",
    "Calculating the likelihood of an observation sequence from an HMM network is performed with the [forward algorithm](https://en.wikipedia.org/wiki/Forward_algorithm). Pomegranate provides the the `HMM.forward()` method to calculate the full matrix showing the likelihood of aligning each observation to each state in the HMM, and the `HMM.log_probability()` method to calculate the cumulative likelihood over all possible hidden state paths that the specified model generated the observation sequence.\n",
    "\n",
    "Fill in the code in the next section with a sample observation sequence and then use the `forward()` and `log_probability()` methods to evaluate the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward algorithm results:\n",
      "Observations: ['yes', 'no', 'yes']\n",
      "Converted to indices: [1, 0, 1]\n",
      "Forward matrix shape: torch.Size([1, 3, 2])\n",
      "\n",
      "The likelihood over all possible paths of this model producing the sequence ['yes', 'no', 'yes'] is 3.46%\n"
     ]
    }
   ],
   "source": [
    "# input a sequence of 'yes'/'no' values in the list below for testing\n",
    "observations = ['yes', 'no', 'yes']\n",
    "\n",
    "assert len(observations) > 0, \"You need to choose a sequence of 'yes'/'no' observations to test\"\n",
    "\n",
    "# Convert string observations to numerical indices\n",
    "# Based on our emission_probs: [no, yes] = [0, 1]\n",
    "obs_to_idx = {'no': 0, 'yes': 1}\n",
    "obs_indices = [obs_to_idx[obs] for obs in observations]\n",
    "\n",
    "# Convert to 3D tensor: (batch_size, sequence_length, feature_dim)\n",
    "obs_tensor = torch.tensor(obs_indices).unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "# Calculate forward probabilities\n",
    "forward_matrix = model.forward(obs_tensor)\n",
    "\n",
    "# Calculate log probability\n",
    "log_prob = model.log_probability(obs_tensor)\n",
    "probability_percentage = torch.exp(log_prob).item()\n",
    "\n",
    "# Display results\n",
    "state_names = [\"Sunny\", \"Rainy\"]\n",
    "print(\"Forward algorithm results:\")\n",
    "print(f\"Observations: {observations}\")\n",
    "print(f\"Converted to indices: {obs_indices}\")\n",
    "print(f\"Forward matrix shape: {forward_matrix.shape}\")\n",
    "print(f\"\\nThe likelihood over all possible paths of this model producing the sequence {observations} is {100 * probability_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION: Decoding the Most Likely Hidden State Sequence\n",
    "\n",
    "The [Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm) calculates the single path with the highest likelihood to produce a specific observation sequence. Pomegranate provides the `HMM.viterbi()` method to calculate both the hidden state sequence and the corresponding likelihood of the viterbi path.\n",
    "\n",
    "This is called \"decoding\" because we use the observation sequence to decode the corresponding hidden state sequence. In the part of speech tagging problem, the hidden states map to parts of speech and the observations map to sentences. Given a sentence, Viterbi decoding finds the most likely sequence of part of speech tags corresponding to the sentence.\n",
    "\n",
    "Fill in the code in the next section with the same sample observation sequence you used above, and then use the `model.viterbi()` method to calculate the likelihood and most likely state sequence. Compare the Viterbi likelihood against the forward algorithm likelihood for the observation sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most likely weather sequence to have generated these observations is ['Rainy', 'Sunny', 'Rainy'] at 3.46%.\n"
     ]
    }
   ],
   "source": [
    "# input a sequence of 'yes'/'no' values in the list below for testing\n",
    "observations = ['yes', 'no', 'yes']\n",
    "\n",
    "# Convert string observations to numerical indices\n",
    "obs_to_idx = {'no': 0, 'yes': 1}\n",
    "obs_indices = [obs_to_idx[obs] for obs in observations]\n",
    "\n",
    "# Convert to 3D tensor: (batch_size, sequence_length, feature_dim)\n",
    "obs_tensor = torch.tensor(obs_indices).unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "# Use predict method for Viterbi decoding\n",
    "predicted_states = model.predict(obs_tensor)\n",
    "\n",
    "# Convert predictions back to state names\n",
    "state_names = [\"Sunny\", \"Rainy\"]\n",
    "predicted_weather = [state_names[idx.item()] for idx in predicted_states[0]]\n",
    "\n",
    "# Calculate log probability for the sequence\n",
    "log_prob = model.log_probability(obs_tensor)\n",
    "likelihood_percentage = torch.exp(log_prob).item() * 100\n",
    "\n",
    "print(\"The most likely weather sequence to have generated \" + \\\n",
    "      \"these observations is {} at {:.2f}%.\"\n",
    "      .format(predicted_weather, likelihood_percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward likelihood vs Viterbi likelihood\n",
    "Run the cells below to see the likelihood of each sequence of observations with length 3, and compare with the viterbi path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The likelihood of observing ['no', 'no', 'yes'] if the weather sequence is...\n",
      "\t('Sunny', 'Sunny', 'Sunny') is 2.59%\n",
      "\t('Sunny', 'Sunny', 'Rainy') is 5.18%\n",
      "\t('Sunny', 'Rainy', 'Sunny') is 0.07%\n",
      "\t('Sunny', 'Rainy', 'Rainy') is 0.86%\n",
      "\t('Rainy', 'Sunny', 'Sunny') is 0.29%\n",
      "\t('Rainy', 'Sunny', 'Rainy') is 0.58%\n",
      "\t('Rainy', 'Rainy', 'Sunny') is 0.05%\n",
      "\t('Rainy', 'Rainy', 'Rainy') is 0.58%\n",
      "\n",
      "The total likelihood of observing ['no', 'no', 'yes'] over all possible paths is 10.20%\n",
      "Model calculated likelihood: 5.10%\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "observations = ['no', 'no', 'yes']\n",
    "\n",
    "# Convert observations to indices for the model\n",
    "obs_to_idx = {'no': 0, 'yes': 1}\n",
    "obs_indices = [obs_to_idx[obs] for obs in observations]\n",
    "obs_tensor = torch.tensor(obs_indices).unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "# Get the Viterbi probability using predict method\n",
    "predicted_states = model.predict(obs_tensor)\n",
    "log_prob = model.log_probability(obs_tensor)\n",
    "vprob = torch.exp(log_prob).item()\n",
    "\n",
    "# Manual calculation for comparison\n",
    "p = {'Sunny': {'Sunny': np.log(.8), 'Rainy': np.log(.2)}, 'Rainy': {'Sunny': np.log(.4), 'Rainy': np.log(.6)}}\n",
    "e = {'Sunny': {'yes': np.log(.1), 'no': np.log(.9)}, 'Rainy':{'yes':np.log(.8), 'no':np.log(.2)}}\n",
    "o = observations\n",
    "k = []\n",
    "\n",
    "print(\"The likelihood of observing {} if the weather sequence is...\".format(o))\n",
    "for s in product(*[['Sunny', 'Rainy']]*3):\n",
    "    prob = np.exp(np.log(.5)+e[s[0]][o[0]] + p[s[0]][s[1]] + e[s[1]][o[1]] + p[s[1]][s[2]] + e[s[2]][o[2]])\n",
    "    k.append(prob)\n",
    "    # Check if this is close to the model's total probability (not individual path)\n",
    "    print(\"\\t{} is {:.2f}%\".format(s, 100 * prob))\n",
    "\n",
    "print(\"\\nThe total likelihood of observing {} over all possible paths is {:.2f}%\".format(o, 100*sum(k)))\n",
    "print(\"Model calculated likelihood: {:.2f}%\".format(100 * vprob))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
